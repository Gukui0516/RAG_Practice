import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import json
import re
from pathlib import Path
from typing import List, Dict
from dotenv import load_dotenv
from collections import defaultdict

from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_openai import AzureOpenAIEmbeddings

load_dotenv()

BASE_DIR = Path(__file__).resolve().parent.parent
CHUNKS_PATH = BASE_DIR / "data" / "chunks.json"
CHROMA_DIR = BASE_DIR / "data" / "chroma_db" / "ev6"

embedding_model = AzureOpenAIEmbeddings(
    azure_deployment=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT"),
    azure_endpoint=os.getenv("AZURE_OPENAI_API_BASE"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version=os.getenv("AZURE_OPENAI_API_VERSION")
)

# âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜


def clean_text(text: str) -> str:
    text = text.replace('\u0001', ' ')
    text = re.sub(
        r'\d{2,4}\.\s?\d{1,2}\.\s?\d{1,2}\.(\s?(ì˜¤ì „|ì˜¤í›„)?\s?\d{1,2}:\d{2})?', '', text)
    text = re.sub(r'(https?://\S+|www\.\S+)', '', text)
    text = re.sub(r'\b[\w.-]+\.kia\.com\S*', '', text)
    text = re.sub(r'\b\d+/\d+\b', '', text)  # ìŠ¬ë¼ì´ë“œ ë²ˆí˜¸ ì œê±°
    text = re.sub(r'\s{2,}', ' ', text)
    return text.strip()

# âœ… ë¬¸ì„œ ë‹¨ìœ„ ë³‘í•© í›„ LangChain Document ë³€í™˜


def convert_to_documents(chunks: List[Dict]) -> List[Document]:
    doc_map = defaultdict(lambda: {"text": [], "image_paths": [], "meta": {}})

    for chunk in chunks:
        text = clean_text(chunk.get("text", ""))
        if not text:
            continue

        key = (chunk.get("section", "").strip().lower(),
               chunk.get("document", ""))
        doc_map[key]["text"].append(text)
        doc_map[key]["image_paths"].extend(chunk.get("image_paths", []))
        doc_map[key]["meta"] = {
            "section": chunk.get("section", "").strip().lower(),
            "document": chunk.get("document", ""),
            "source": chunk.get("source", ""),
            "category": chunk.get("category", "")
        }

    docs = []
    for (_, _), content in doc_map.items():
        full_text = "\n\n".join(content["text"])
        metadata = {
            **content["meta"],
            "image_paths": ", ".join(content["image_paths"]),
            "image_names": ", ".join([os.path.basename(p) for p in content["image_paths"]])
        }
        docs.append(Document(page_content=full_text, metadata=metadata))

    return docs

# âœ… JSON íŒŒì¼ ë¡œë“œ


def load_chunks(json_path: Path) -> List[Dict]:
    with open(json_path, "r", encoding="utf-8") as f:
        return json.load(f)

# âœ… ë²¡í„° DB ì €ì¥


def store_to_chroma(documents: List[Document], persist_path: Path):
    vectordb = Chroma.from_documents(
        documents=documents,
        embedding=embedding_model,
        persist_directory=str(persist_path)
    )
    vectordb.persist()
    print(f"âœ… ChromaDB ì €ì¥ ì™„ë£Œ â†’ {persist_path}")


if __name__ == "__main__":
    print("ğŸ“¦ chunks.json ë¡œë”© ì¤‘...")
    chunks = load_chunks(CHUNKS_PATH)
    print(f"ğŸ”¹ ì´ ì²­í¬ ìˆ˜: {len(chunks)}")

    print("ğŸ§  ë¬¸ì„œ ë‹¨ìœ„ë¡œ ì„ë² ë”© ì¤€ë¹„ ì¤‘...")
    documents = convert_to_documents(chunks)
    print(f"âœ… ë³€í™˜ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")

    if documents:
        print(f"ğŸ” ì˜ˆì‹œ ë¬¸ì„œ ë‚´ìš©: {documents[0].page_content[:100]}...")
        print(f"ğŸ” ì˜ˆì‹œ ë©”íƒ€ë°ì´í„°: {documents[0].metadata}")

    print("ğŸ’¾ ChromaDB ì €ì¥ ì¤‘...")
    store_to_chroma(documents, CHROMA_DIR)
